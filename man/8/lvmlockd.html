<!-- Creator     : groff version 1.22.4 -->
<!-- CreationDate: Mon May 29 22:57:06 2023 -->
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<meta name="generator" content="groff -Thtml, see www.gnu.org">
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="Content-Style" content="text/css">
<style type="text/css">
       p       { margin-top: 0; margin-bottom: 0; vertical-align: top }
       pre     { margin-top: 0; margin-bottom: 0; vertical-align: top }
       table   { margin-top: 0; margin-bottom: 0; vertical-align: top }
       h1      { text-align: center }
</style>
<title>LVMLOCKD</title>

</head>
<body>
<h1>lvmlockd</h1>



<h2>NAME
<a name="NAME"></a>
</h2>


<p style="margin-left:11%; margin-top: 1em">lvmlockd
&mdash; LVM locking daemon</p>

<h2>SYNOPSIS
<a name="SYNOPSIS"></a>
</h2>



<p style="margin-left:11%; margin-top: 1em"><b>lvmlockd</b>
[<i>options</i>]</p>

<h2>DESCRIPTION
<a name="DESCRIPTION"></a>
</h2>


<p style="margin-left:11%; margin-top: 1em">LVM commands
use lvmlockd to coordinate access to shared storage. <br>
When LVM is used on devices shared by multiple hosts, locks
will: <br>
&bull; coordinate reading and writing of LVM metadata <br>
&bull; validate caching of LVM metadata <br>
&bull; prevent conflicting activation of logical volumes
<br>
lvmlockd uses an external lock manager to perform basic
locking.</p>

<p style="margin-left:11%; margin-top: 1em">Lock manager
(lock type) options are: <br>
&bull; sanlock: places locks on disk within LVM storage.
<br>
&bull; dlm: uses network communication and a cluster
manager.</p>

<h2>OPTIONS
<a name="OPTIONS"></a>
</h2>



<p style="margin-left:11%; margin-top: 1em"><b>&minus;h</b>|<b>&minus;&minus;help</b></p>

<p style="margin-left:22%;">Show this help information.</p>


<p style="margin-left:11%;"><b>&minus;V</b>|<b>&minus;&minus;version</b></p>

<p style="margin-left:22%;">Show version of lvmlockd.</p>


<p style="margin-left:11%;"><b>&minus;T</b>|<b>&minus;&minus;test</b></p>

<p style="margin-left:22%;">Test mode, do not call lock
manager.</p>


<p style="margin-left:11%;"><b>&minus;f</b>|<b>&minus;&minus;foreground</b></p>

<p style="margin-left:22%;">Don&rsquo;t fork.</p>


<p style="margin-left:11%;"><b>&minus;D</b>|<b>&minus;&minus;daemon&minus;debug</b></p>

<p style="margin-left:22%;">Don&rsquo;t fork and print
debugging to stdout.</p>


<p style="margin-left:11%;"><b>&minus;p</b>|<b>&minus;&minus;pid&minus;file</b>
<i>path</i></p>

<p style="margin-left:22%;">Set path to the pid file.</p>


<p style="margin-left:11%;"><b>&minus;s</b>|<b>&minus;&minus;socket&minus;path</b>
<i>path</i></p>

<p style="margin-left:22%;">Set path to the socket to
listen on.</p>


<p style="margin-left:11%;"><b>&minus;&minus;adopt&minus;file</b>
<i>path</i></p>

<p style="margin-left:22%;">Set path to the adopt file.</p>


<p style="margin-left:11%;"><b>&minus;S</b>|<b>&minus;&minus;syslog&minus;priority
err</b>|<b>warning</b>|<b>debug</b></p>

<p style="margin-left:22%;">Write log messages from this
level up to syslog.</p>


<p style="margin-left:11%;"><b>&minus;g</b>|<b>&minus;&minus;gl&minus;type
sanlock</b>|<b>dlm</b></p>

<p style="margin-left:22%;">Set global lock type to be
sanlock or dlm.</p>


<p style="margin-left:11%;"><b>&minus;i</b>|<b>&minus;&minus;host&minus;id</b>
<i>num</i></p>

<p style="margin-left:22%;">Set the local sanlock host
id.</p>


<p style="margin-left:11%;"><b>&minus;F</b>|<b>&minus;&minus;host&minus;id&minus;file</b>
<i>path</i></p>

<p style="margin-left:22%;">A file containing the local
sanlock host_id.</p>


<p style="margin-left:11%;"><b>&minus;o</b>|<b>&minus;&minus;sanlock&minus;timeout</b>
<i>seconds</i></p>

<p style="margin-left:22%;">Override the default sanlock
I/O timeout.</p>


<p style="margin-left:11%;"><b>&minus;A</b>|<b>&minus;&minus;adopt
0</b>|<b>1</b></p>

<p style="margin-left:22%;">Enable (1) or disable (0) lock
adoption.</p>

<h2>USAGE
<a name="USAGE"></a>
</h2>


<p style="margin-left:11%; margin-top: 1em"><b>Initial set
up</b> <br>
Setting up LVM to use lvmlockd and a shared VG for the first
time includes some one time set up steps:</p>

<p style="margin-left:11%; margin-top: 1em"><b>1. choose a
lock manager</b> <i><br>
dlm</i> <br>
If dlm (or corosync) are already being used by other cluster
software, then select dlm. dlm uses corosync which requires
additional configuration beyond the scope of this document.
See corosync and dlm documentation for instructions on
configuration, set up and usage.</p>

<p style="margin-left:11%; margin-top: 1em"><i>sanlock</i>
<br>
Choose sanlock if dlm/corosync are not otherwise required.
sanlock does not depend on any clustering software or
configuration.</p>

<p style="margin-left:11%; margin-top: 1em"><b>2. configure
hosts to use lvmlockd</b> <br>
On all hosts running lvmlockd, configure lvm.conf: <br>
use_lvmlockd = 1</p>

<p style="margin-left:11%; margin-top: 1em"><i>sanlock</i>
<br>
Assign each host a unique host_id in the range 1&minus;2000
by setting <br>
/etc/lvm/lvmlocal.conf local/host_id</p>

<p style="margin-left:11%; margin-top: 1em"><b>3. start
lvmlockd</b> <br>
Start the lvmlockd daemon. <br>
Use systemctl, a cluster resource agent, or run directly,
e.g. <br>
systemctl start lvmlockd</p>

<p style="margin-left:11%; margin-top: 1em"><b>4. start
lock manager</b> <i><br>
sanlock</i> <br>
Start the sanlock and wdmd daemons. <br>
Use systemctl or run directly, e.g. <br>
systemctl start wdmd sanlock</p>

<p style="margin-left:11%; margin-top: 1em"><i>dlm</i> <br>
Start the dlm and corosync daemons. <br>
Use systemctl, a cluster resource agent, or run directly,
e.g. <br>
systemctl start corosync dlm</p>

<p style="margin-left:11%; margin-top: 1em"><b>5. create VG
on shared devices</b> <br>
vgcreate &minus;&minus;shared &lt;vgname&gt;
&lt;devices&gt;</p>

<p style="margin-left:11%; margin-top: 1em">The shared
option sets the VG lock type to sanlock or dlm depending on
which lock manager is running. LVM commands acquire locks
from lvmlockd, and lvmlockd uses the chosen lock
manager.</p>

<p style="margin-left:11%; margin-top: 1em"><b>6. start VG
on all hosts</b> <br>
vgchange &minus;&minus;lock&minus;start</p>

<p style="margin-left:11%; margin-top: 1em">Shared VGs must
be started before they are used. Starting the VG performs
lock manager initialization that is necessary to begin using
locks (i.e. creating and joining a lockspace). Starting the
VG may take some time, and until the start completes the VG
may not be modified or activated.</p>

<p style="margin-left:11%; margin-top: 1em"><b>7. create
and activate LVs</b> <br>
Standard lvcreate and lvchange commands are used to create
and activate LVs in a shared VG.</p>

<p style="margin-left:11%; margin-top: 1em">An LV activated
exclusively on one host cannot be activated on another. When
multiple hosts need to use the same LV concurrently, the LV
can be activated with a shared lock (see lvchange options
&minus;aey vs &minus;asy.) (Shared locks are disallowed for
certain LV types that cannot be used from multiple
hosts.)</p>

<p style="margin-left:11%; margin-top: 1em"><b>Normal start
up and shut down</b> <br>
After initial set up, start up and shut down include the
following steps. They can be performed directly or may be
automated using systemd or a cluster resource
manager/agents.</p>

<p style="margin-left:11%; margin-top: 1em">&bull; start
lvmlockd <br>
&bull; start lock manager <br>
&bull; vgchange &minus;&minus;lock&minus;start <br>
&bull; activate LVs in shared VGs</p>

<p style="margin-left:11%; margin-top: 1em">The shut down
sequence is the reverse:</p>

<p style="margin-left:11%; margin-top: 1em">&bull;
deactivate LVs in shared VGs <br>
&bull; vgchange &minus;&minus;lock&minus;stop <br>
&bull; stop lock manager <br>
&bull; stop lvmlockd</p>

<h2>TOPICS
<a name="TOPICS"></a>
</h2>


<p style="margin-left:11%; margin-top: 1em"><b>Protecting
VGs on shared devices</b> <br>
The following terms are used to describe the different ways
of accessing VGs on shared devices.</p>

<p style="margin-left:11%; margin-top: 1em"><i>shared
VG</i></p>

<p style="margin-left:11%; margin-top: 1em">A shared VG
exists on shared storage that is visible to multiple hosts.
LVM acquires locks through lvmlockd to coordinate access to
shared VGs. A shared VG has lock_type &quot;dlm&quot; or
&quot;sanlock&quot;, which specifies the lock manager
lvmlockd will use.</p>

<p style="margin-left:11%; margin-top: 1em">When the lock
manager for the lock type is not available (e.g. not started
or failed), lvmlockd is unable to acquire locks for LVM
commands. In this situation, LVM commands are only allowed
to read and display the VG; changes and activation will
fail.</p>

<p style="margin-left:11%; margin-top: 1em"><i>local
VG</i></p>

<p style="margin-left:11%; margin-top: 1em">A local VG is
meant to be used by a single host. It has no lock type or
lock type &quot;none&quot;. A local VG typically exists on
local (non-shared) devices and cannot be used concurrently
from different hosts.</p>

<p style="margin-left:11%; margin-top: 1em">If a local VG
does exist on shared devices, it should be owned by a single
host by having the system ID set, see <b>lvmsystemid</b>(7).
The host with a matching system ID can use the local VG and
other hosts will ignore it. A VG with no lock type and no
system ID should be excluded from all but one host using
lvm.conf filters. Without any of these protections, a local
VG on shared devices can be easily damaged or destroyed.</p>

<p style="margin-left:11%; margin-top: 1em"><i>clvm
VG</i></p>

<p style="margin-left:11%; margin-top: 1em">A clvm VG (or
clustered VG) is a VG on shared storage (like a shared VG)
that requires clvmd for clustering and locking. See below
for converting a clvm/clustered VG to a shared VG.</p>

<p style="margin-left:11%; margin-top: 1em"><b>Shared VGs
from hosts not using lvmlockd</b> <br>
Hosts that do not use shared VGs will not be running
lvmlockd. In this case, shared VGs that are still visible to
the host will be ignored (like foreign VGs, see
<b>lvmsystemid</b>(7)).</p>

<p style="margin-left:11%; margin-top: 1em">The
&minus;&minus;shared option for reporting and display
commands causes shared VGs to be displayed on a host not
using lvmlockd, like the &minus;&minus;foreign option does
for foreign VGs.</p>

<p style="margin-left:11%; margin-top: 1em"><b>Creating the
first sanlock VG</b> <br>
When use_lvmlockd is first enabled in lvm.conf, and before
the first sanlock VG is created, no global lock will exist.
In this initial state, LVM commands try and fail to acquire
the global lock, producing a warning, and some commands are
disallowed. Once the first sanlock VG is created, the global
lock will be available, and LVM will be fully
operational.</p>

<p style="margin-left:11%; margin-top: 1em">When a new
sanlock VG is created, its lockspace is automatically
started on the host that creates it. Other hosts need to run
&rsquo;vgchange &minus;&minus;lock&minus;start&rsquo; to
start the new VG before they can use it.</p>

<p style="margin-left:11%; margin-top: 1em">Creating the
first sanlock VG is not protected by locking, so it requires
special attention. This is because sanlock locks exist on
storage within the VG, so they are not available until after
the VG is created. The first sanlock VG that is created will
automatically contain the &quot;global lock&quot;. Be aware
of the following special considerations:</p>

<table width="100%" border="0" rules="none" frame="void"
       cellspacing="0" cellpadding="0">
<tr valign="top" align="left">
<td width="11%"></td>
<td width="1%">


<p>&bull;</p></td>
<td width="2%"></td>
<td width="86%">


<p>The first vgcreate command needs to be given the path to
a device that has not yet been initialized with pvcreate.
The pvcreate initialization will be done by vgcreate. This
is because the pvcreate command requires the global lock,
which will not be available until after the first sanlock VG
is created.</p></td></tr>
<tr valign="top" align="left">
<td width="11%"></td>
<td width="1%">


<p>&bull;</p></td>
<td width="2%"></td>
<td width="86%">


<p>Because the first sanlock VG will contain the global
lock, this VG needs to be accessible to all hosts that will
use sanlock shared VGs. All hosts will need to use the
global lock from the first sanlock VG.</p></td></tr>
<tr valign="top" align="left">
<td width="11%"></td>
<td width="1%">


<p>&bull;</p></td>
<td width="2%"></td>
<td width="86%">


<p>The device and VG name used by the initial vgcreate will
not be protected from concurrent use by another vgcreate on
another host.</p></td></tr>
</table>

<p style="margin-left:11%; margin-top: 1em">See below for
more information about managing the sanlock global lock.</p>

<p style="margin-left:11%; margin-top: 1em"><b>Using shared
VGs</b> <br>
In the &rsquo;vgs&rsquo; command, shared VGs are indicated
by &quot;s&quot; (for shared) in the sixth attr field, and
by &quot;shared&quot; in the &quot;&minus;&minus;options
shared&quot; report field. The specific lock type and lock
args for a shared VG can be displayed with &rsquo;vgs
&minus;o+locktype,lockargs&rsquo;.</p>

<p style="margin-left:11%; margin-top: 1em">Shared VGs need
to be &quot;started&quot; and &quot;stopped&quot;, unlike
other types of VGs. See the following section for a full
description of starting and stopping.</p>

<p style="margin-left:11%; margin-top: 1em">Removing a
shared VG will fail if other hosts have the VG started. Run
vgchange &minus;&minus;lock&minus;stop &lt;vgname&gt; on all
other hosts before vgremove. (It may take several seconds
before vgremove recognizes that all hosts have stopped a
sanlock VG.)</p>

<p style="margin-left:11%; margin-top: 1em"><b>Starting and
stopping VGs</b> <br>
Starting a shared VG (vgchange
&minus;&minus;lock&minus;start) causes the lock manager to
start (join) the lockspace for the VG on the host where it
is run. This makes locks for the VG available to LVM
commands on the host. Before a VG is started, only LVM
commands that read/display the VG are allowed to continue
without locks (and with a warning).</p>

<p style="margin-left:11%; margin-top: 1em">Stopping a
shared VG (vgchange &minus;&minus;lock&minus;stop) causes
the lock manager to stop (leave) the lockspace for the VG on
the host where it is run. This makes locks for the VG
inaccessible to the host. A VG cannot be stopped while it
has active LVs.</p>

<p style="margin-left:11%; margin-top: 1em">When using the
lock type sanlock, starting a VG can take a long time
(potentially minutes if the host was previously shut down
without cleanly stopping the VG.)</p>

<p style="margin-left:11%; margin-top: 1em">A shared VG can
be started after all the following are true:</p>

<p style="margin-left:11%; margin-top: 1em">&bull; lvmlockd
is running <br>
&bull; the lock manager is running <br>
&bull; the VG&rsquo;s devices are visible on the system</p>

<p style="margin-left:11%; margin-top: 1em">A shared VG can
be stopped if all LVs are deactivated.</p>

<p style="margin-left:11%; margin-top: 1em">All shared VGs
can be started/stopped using: <br>
vgchange &minus;&minus;lock&minus;start <br>
vgchange &minus;&minus;lock&minus;stop</p>

<p style="margin-left:11%; margin-top: 1em">Individual VGs
can be started/stopped using: <br>
vgchange &minus;&minus;lock&minus;start &lt;vgname&gt; ...
<br>
vgchange &minus;&minus;lock&minus;stop &lt;vgname&gt;
...</p>

<p style="margin-left:11%; margin-top: 1em">To make
vgchange not wait for start to complete: <br>
vgchange &minus;&minus;lock&minus;start
&minus;&minus;lock&minus;opt nowait ...</p>

<p style="margin-left:11%; margin-top: 1em">lvmlockd can be
asked directly to stop all lockspaces: <br>
lvmlockctl &minus;S|&minus;&minus;stop&minus;lockspaces</p>

<p style="margin-left:11%; margin-top: 1em">To start only
selected shared VGs, use the lvm.conf
activation/lock_start_list. When defined, only VG names in
this list are started by vgchange. If the list is not
defined (the default), all visible shared VGs are started.
To start only &quot;vg1&quot;, use the following lvm.conf
configuration:</p>

<p style="margin-left:11%; margin-top: 1em">activation {
<br>
lock_start_list = [ &quot;vg1&quot; ] <br>
... <br>
}</p>

<p style="margin-left:11%; margin-top: 1em"><b>Internal
command locking</b> <br>
To optimize the use of LVM with lvmlockd, be aware of the
three kinds of locks and when they are used:</p>

<p style="margin-left:11%; margin-top: 1em"><i>Global
lock</i></p>

<p style="margin-left:11%; margin-top: 1em">The global lock
is associated with global information, which is information
not isolated to a single VG. This includes:</p>

<p style="margin-left:11%; margin-top: 1em">&bull; The
global VG namespace. <br>
&bull; The set of orphan PVs and unused devices. <br>
&bull; The properties of orphan PVs, e.g. PV size.</p>

<p style="margin-left:11%; margin-top: 1em">The global lock
is acquired in shared mode by commands that read this
information, or in exclusive mode by commands that change
it. For example, the command &rsquo;vgs&rsquo; acquires the
global lock in shared mode because it reports the list of
all VG names, and the vgcreate command acquires the global
lock in exclusive mode because it creates a new VG name, and
it takes a PV from the list of unused PVs.</p>

<p style="margin-left:11%; margin-top: 1em">When an LVM
command is given a tag argument, or uses select, it must
read all VGs to match the tag or selection, which causes the
global lock to be acquired.</p>

<p style="margin-left:11%; margin-top: 1em"><i>VG
lock</i></p>

<p style="margin-left:11%; margin-top: 1em">A VG lock is
associated with each shared VG. The VG lock is acquired in
shared mode to read the VG and in exclusive mode to change
the VG or activate LVs. This lock serializes access to a VG
with all other LVM commands accessing the VG from all
hosts.</p>

<p style="margin-left:11%; margin-top: 1em">The command
&rsquo;vgs &lt;vgname&gt;&rsquo; does not acquire the global
lock (it does not need the list of all VG names), but will
acquire the VG lock on each VG name argument.</p>

<p style="margin-left:11%; margin-top: 1em"><i>LV
lock</i></p>

<p style="margin-left:11%; margin-top: 1em">An LV lock is
acquired before the LV is activated, and is released after
the LV is deactivated. If the LV lock cannot be acquired,
the LV is not activated. (LV locks are persistent and remain
in place when the activation command is done. Global and VG
locks are transient, and are held only while an LVM command
is running.)</p>

<p style="margin-left:11%; margin-top: 1em"><i>lock
retries</i></p>

<p style="margin-left:11%; margin-top: 1em">If a request
for a global or VG lock fails due to a lock conflict with
another host, lvmlockd automatically retries for a short
time before returning a failure to the LVM command. If those
retries are insufficient, the LVM command will retry the
entire lock request a number of times specified by
global/lvmlockd_lock_retries before failing. If a request
for an LV lock fails due to a lock conflict, the command
fails immediately.</p>

<p style="margin-left:11%; margin-top: 1em"><b>Managing the
global lock in sanlock VGs</b> <br>
The global lock exists in one of the sanlock VGs. The first
sanlock VG created will contain the global lock. Subsequent
sanlock VGs will each contain a disabled global lock that
can be enabled later if necessary.</p>

<p style="margin-left:11%; margin-top: 1em">The VG
containing the global lock must be visible to all hosts
using sanlock VGs. For this reason, it can be useful to
create a small sanlock VG, visible to all hosts, and
dedicated to just holding the global lock. While not
required, this strategy can help to avoid difficulty in the
future if VGs are moved or removed.</p>

<p style="margin-left:11%; margin-top: 1em">The vgcreate
command typically acquires the global lock, but in the case
of the first sanlock VG, there will be no global lock to
acquire until the first vgcreate is complete. So, creating
the first sanlock VG is a special case that skips the global
lock.</p>

<p style="margin-left:11%; margin-top: 1em">vgcreate
determines that it&rsquo;s creating the first sanlock VG
when no other sanlock VGs are visible on the system. It is
possible that other sanlock VGs do exist, but are not
visible when vgcreate checks for them. In this case,
vgcreate will create a new sanlock VG with the global lock
enabled. When the another VG containing a global lock
appears, lvmlockd will then see more than one VG with a
global lock enabled. LVM commands will report that there are
duplicate global locks.</p>

<p style="margin-left:11%; margin-top: 1em">If the
situation arises where more than one sanlock VG contains a
global lock, the global lock should be manually disabled in
all but one of them with the command:</p>

<p style="margin-left:11%; margin-top: 1em">lvmlockctl
&minus;&minus;gl&minus;disable &lt;vgname&gt;</p>

<p style="margin-left:11%; margin-top: 1em">(The one VG
with the global lock enabled must be visible to all
hosts.)</p>

<p style="margin-left:11%; margin-top: 1em">An opposite
problem can occur if the VG holding the global lock is
removed. In this case, no global lock will exist following
the vgremove, and subsequent LVM commands will fail to
acquire it. In this case, the global lock needs to be
manually enabled in one of the remaining sanlock VGs with
the command:</p>

<p style="margin-left:11%; margin-top: 1em">lvmlockctl
&minus;&minus;gl&minus;enable &lt;vgname&gt;</p>

<p style="margin-left:11%; margin-top: 1em">(Using a small
sanlock VG dedicated to holding the global lock can avoid
the case where the global lock must be manually enabled
after a vgremove.)</p>

<p style="margin-left:11%; margin-top: 1em"><b>Internal
lvmlock LV</b> <br>
A sanlock VG contains a hidden LV called &quot;lvmlock&quot;
that holds the sanlock locks. vgreduce cannot yet remove the
PV holding the lvmlock LV. To remove this PV, change the VG
lock type to &quot;none&quot;, run vgreduce, then change the
VG lock type back to &quot;sanlock&quot;. Similarly, pvmove
cannot be used on a PV used by the lvmlock LV.</p>

<p style="margin-left:11%; margin-top: 1em">To place the
lvmlock LV on a specific device, create the VG with only
that device, then use vgextend to add other devices.</p>

<p style="margin-left:11%; margin-top: 1em"><b>LV
activation</b> <br>
In a shared VG, LV activation involves locking through
lvmlockd, and the following values are possible with
lvchange/vgchange &minus;a:</p>

<table width="100%" border="0" rules="none" frame="void"
       cellspacing="0" cellpadding="0">
<tr valign="top" align="left">
<td width="11%"></td>
<td width="6%">


<p style="margin-top: 1em"><b>y</b>|<b>ey</b></p></td>
<td width="5%"></td>
<td width="78%">


<p style="margin-top: 1em">The command activates the LV in
exclusive mode, allowing a single host to activate the LV.
Before activating the LV, the command uses lvmlockd to
acquire an exclusive lock on the LV. If the lock cannot be
acquired, the LV is not activated and an error is reported.
This would happen if the LV is active on another host.</p></td></tr>
<tr valign="top" align="left">
<td width="11%"></td>
<td width="6%">


<p><b>sy</b></p></td>
<td width="5%"></td>
<td width="78%">


<p>The command activates the LV in shared mode, allowing
multiple hosts to activate the LV concurrently. Before
activating the LV, the command uses lvmlockd to acquire a
shared lock on the LV. If the lock cannot be acquired, the
LV is not activated and an error is reported. This would
happen if the LV is active exclusively on another host. If
the LV type prohibits shared access, such as a snapshot, the
command will report an error and fail. The shared mode is
intended for a multi-host/cluster application or file
system. LV types that cannot be used concurrently from
multiple hosts include thin, cache, raid, mirror, and
snapshot.</p> </td></tr>
<tr valign="top" align="left">
<td width="11%"></td>
<td width="6%">


<p><b>n</b></p></td>
<td width="5%"></td>
<td width="78%">


<p>The command deactivates the LV. After deactivating the
LV, the command uses lvmlockd to release the current lock on
the LV.</p></td></tr>
</table>

<p style="margin-left:11%; margin-top: 1em"><b>Manually
repairing a shared VG</b> <br>
Some failure conditions may not be repairable while the VG
has a shared lock type. In these cases, it may be possible
to repair the VG by forcibly changing the lock type to
&quot;none&quot;. This is done by adding
&quot;&minus;&minus;lock&minus;opt force&quot; to the normal
command for changing the lock type: vgchange
&minus;&minus;lock&minus;type none VG. The VG lockspace
should first be stopped on all hosts, and be certain that no
hosts are using the VG before this is done.</p>

<p style="margin-left:11%; margin-top: 1em"><b>Recover from
lost PV holding sanlock locks</b> <br>
In a sanlock VG, the sanlock locks are held on the hidden
&quot;lvmlock&quot; LV. If the PV holding this LV is lost, a
new lvmlock LV needs to be created. To do this, ensure no
hosts are using the VG, then forcibly change the lock type
to &quot;none&quot; (see above). Then change the lock type
back to &quot;sanlock&quot; with the normal command for
changing the lock type: vgchange
&minus;&minus;lock&minus;type sanlock VG. This recreates the
internal lvmlock LV with the necessary locks.</p>

<p style="margin-left:11%; margin-top: 1em"><b>Locking
system failures <br>
lvmlockd failure</b></p>

<p style="margin-left:11%; margin-top: 1em">If lvmlockd
fails or is killed while holding locks, the locks are
orphaned in the lock manager. Orphaned locks must be cleared
or adopted before the associated resources can be accessed
normally. If lock adoption is enabled, lvmlockd keeps a
record of locks in the adopt-file. A subsequent instance of
lvmlockd will then adopt locks orphaned by the previous
instance. Adoption must be enabled in both instances
(&minus;&minus;adopt|&minus;A 1). Without adoption, the lock
manager or host would require a reset to clear orphaned lock
state.</p>


<p style="margin-left:11%; margin-top: 1em"><b>dlm/corosync
failure</b></p>

<p style="margin-left:11%; margin-top: 1em">If dlm or
corosync fail, the clustering system will fence the host
using a method configured within the dlm/corosync clustering
environment.</p>

<p style="margin-left:11%; margin-top: 1em">LVM commands on
other hosts will be blocked from acquiring any locks until
the dlm/corosync recovery process is complete.</p>

<p style="margin-left:11%; margin-top: 1em"><b>sanlock
lease storage failure</b></p>

<p style="margin-left:11%; margin-top: 1em">If the PV under
a sanlock VG&rsquo;s lvmlock LV is disconnected,
unresponsive or too slow, sanlock cannot renew the lease for
the VG&rsquo;s locks. After some time, the lease will
expire, and locks that the host owns in the VG can be
acquired by other hosts. The VG must be forcibly deactivated
on the host with the expiring lease before other hosts can
acquire its locks. This is necessary for data
protection.</p>

<p style="margin-left:11%; margin-top: 1em">When the
sanlock daemon detects that VG storage is lost and the VG
lease is expiring, it runs the command lvmlockctl
&minus;&minus;kill &lt;vgname&gt;. This command emits a
syslog message stating that storage is lost for the VG, and
that LVs in the VG must be immediately deactivated.</p>

<p style="margin-left:11%; margin-top: 1em">If no LVs are
active in the VG, then the VG lockspace will be removed, and
errors will be reported when trying to use the VG. Use the
lvmlockctl &minus;&minus;drop command to clear the stale
lockspace from lvmlockd.</p>

<p style="margin-left:11%; margin-top: 1em">If the VG has
active LVs, they must be quickly deactivated before the
locks expire. After all LVs are deactivated, run lvmlockctl
&minus;&minus;drop &lt;vgname&gt; to clear the expiring
lockspace from lvmlockd.</p>

<p style="margin-left:11%; margin-top: 1em">If all LVs in
the VG are not deactivated within about 40 seconds, sanlock
uses wdmd and the local watchdog to reset the host. The
machine reset is effectively a severe form of
&quot;deactivating&quot; LVs before they can be activated on
other hosts. The reset is considered a better alternative
than having LVs used by multiple hosts at once, which could
easily damage or destroy their content.</p>

<p style="margin-left:11%; margin-top: 1em"><b>sanlock
lease storage failure automation</b></p>

<p style="margin-left:11%; margin-top: 1em">When the
sanlock daemon detects that the lease storage is lost, it
runs the command lvmlockctl &minus;&minus;kill
&lt;vgname&gt;. This lvmlockctl command can be configured to
run another command to forcibly deactivate LVs, taking the
place of the manual process described above. The other
command is configured in the lvm.conf
lvmlockctl_kill_command setting. The VG name is appended to
the end of the command specified.</p>

<p style="margin-left:11%; margin-top: 1em">The
lvmlockctl_kill_command should forcibly deactivate LVs in
the VG, ensuring that existing writes to LVs in the VG are
complete and that further writes to the LVs in the VG will
be rejected. If it is able to do this successfully, it
should exit with success, otherwise it should exit with an
error. If lvmlockctl &minus;&minus;kill gets a successful
result from lvmlockctl_kill_command, it tells lvmlockd to
drop locks for the VG (the equivalent of running lvmlockctl
&minus;&minus;drop). If this completes in time, a machine
reset can be avoided.</p>

<p style="margin-left:11%; margin-top: 1em">One possible
option is to create a script my_vg_kill_script.sh: <br>
#!/bin/bash <br>
VG=$1 <br>
# replace dm table with the error target for top level LVs
<br>
dmsetup wipe_table &minus;S &quot;uuid=~LVM &amp;&amp;
vgname=$VG &amp;&amp; lv_layer=\&quot;\&quot;&quot; <br>
# check that the error target is in place <br>
dmsetup table &minus;c &minus;S &quot;uuid=~LVM &amp;&amp;
vgname=$VG &amp;&amp; lv_layer=\&quot;\&quot;&quot; |grep
&minus;vw error <br>
if [[ $? &minus;ne 0 ]] ; then <br>
exit 0 <br>
fi <br>
exit 1</p>

<p style="margin-left:11%; margin-top: 1em">Set in
lvm.conf: <br>

lvmlockctl_kill_command=&quot;/usr/sbin/my_vg_kill_script.sh&quot;</p>

<p style="margin-left:11%; margin-top: 1em">(The script and
dmsetup commands should be tested with the actual VG to
ensure that all top level LVs are properly disabled.)</p>

<p style="margin-left:11%; margin-top: 1em">If the
lvmlockctl_kill_command is not configured, or fails,
lvmlockctl &minus;&minus;kill will emit syslog messages as
described in the previous section, notifying the user to
manually deactivate the VG before sanlock resets the
machine.</p>

<p style="margin-left:11%; margin-top: 1em"><b>sanlock
daemon failure</b></p>

<p style="margin-left:11%; margin-top: 1em">If the sanlock
daemon fails or exits while a lockspace is started, the
local watchdog will reset the host. This is necessary to
protect any application resources that depend on sanlock
leases.</p>

<p style="margin-left:11%; margin-top: 1em"><b>Changing dlm
cluster name</b> <br>
When a dlm VG is created, the cluster name is saved in the
VG metadata. To use the VG, a host must be in the named dlm
cluster. If the dlm cluster name changes, or the VG is moved
to a new cluster, the dlm cluster name saved in the VG must
also be changed.</p>

<p style="margin-left:11%; margin-top: 1em">To see the dlm
cluster name saved in the VG, use the command: <br>
vgs &minus;o+locktype,lockargs &lt;vgname&gt;</p>

<p style="margin-left:11%; margin-top: 1em">To change the
dlm cluster name in the VG when the VG is still used by the
original cluster:</p>

<table width="100%" border="0" rules="none" frame="void"
       cellspacing="0" cellpadding="0">
<tr valign="top" align="left">
<td width="11%"></td>
<td width="1%">


<p style="margin-top: 1em">&bull;</p></td>
<td width="2%"></td>
<td width="72%">


<p style="margin-top: 1em">Start the VG on the host
changing the lock type</p></td>
<td width="14%">
</td></tr>
</table>

<p style="margin-left:14%;">vgchange
&minus;&minus;lock&minus;start &lt;vgname&gt;</p>

<table width="100%" border="0" rules="none" frame="void"
       cellspacing="0" cellpadding="0">
<tr valign="top" align="left">
<td width="11%"></td>
<td width="1%">


<p style="margin-top: 1em">&bull;</p></td>
<td width="2%"></td>
<td width="48%">


<p style="margin-top: 1em">Stop the VG on all other
hosts:</p> </td>
<td width="38%">
</td></tr>
</table>

<p style="margin-left:14%;">vgchange
&minus;&minus;lock&minus;stop &lt;vgname&gt;</p>

<table width="100%" border="0" rules="none" frame="void"
       cellspacing="0" cellpadding="0">
<tr valign="top" align="left">
<td width="11%"></td>
<td width="1%">


<p style="margin-top: 1em">&bull;</p></td>
<td width="2%"></td>
<td width="86%">


<p style="margin-top: 1em">Change the VG lock type to none
on the host where the VG is started:</p></td></tr>
</table>

<p style="margin-left:14%;">vgchange
&minus;&minus;lock&minus;type none &lt;vgname&gt;</p>

<table width="100%" border="0" rules="none" frame="void"
       cellspacing="0" cellpadding="0">
<tr valign="top" align="left">
<td width="11%"></td>
<td width="1%">


<p style="margin-top: 1em">&bull;</p></td>
<td width="2%"></td>
<td width="86%">


<p style="margin-top: 1em">Change the dlm cluster name on
the hosts or move the VG to the new cluster. The new dlm
cluster must now be running on the host. Verify the new name
by:</p> </td></tr>
</table>

<p style="margin-left:14%;">cat
/sys/kernel/config/dlm/cluster/cluster_name</p>

<table width="100%" border="0" rules="none" frame="void"
       cellspacing="0" cellpadding="0">
<tr valign="top" align="left">
<td width="11%"></td>
<td width="1%">


<p style="margin-top: 1em">&bull;</p></td>
<td width="2%"></td>
<td width="86%">


<p style="margin-top: 1em">Change the VG lock type back to
dlm which sets the new cluster name:</p></td></tr>
</table>

<p style="margin-left:14%;">vgchange
&minus;&minus;lock&minus;type dlm &lt;vgname&gt;</p>

<table width="100%" border="0" rules="none" frame="void"
       cellspacing="0" cellpadding="0">
<tr valign="top" align="left">
<td width="11%"></td>
<td width="1%">


<p style="margin-top: 1em">&bull;</p></td>
<td width="2%"></td>
<td width="49%">


<p style="margin-top: 1em">Start the VG on hosts to use
it:</p> </td>
<td width="37%">
</td></tr>
</table>

<p style="margin-left:14%;">vgchange
&minus;&minus;lock&minus;start &lt;vgname&gt;</p>

<p style="margin-left:11%; margin-top: 1em">To change the
dlm cluster name in the VG when the dlm cluster name has
already been changed on the hosts, or the VG has already
moved to a different cluster:</p>

<table width="100%" border="0" rules="none" frame="void"
       cellspacing="0" cellpadding="0">
<tr valign="top" align="left">
<td width="11%"></td>
<td width="1%">


<p style="margin-top: 1em">&bull;</p></td>
<td width="2%"></td>
<td width="86%">


<p style="margin-top: 1em">Ensure the VG is not being used
by any hosts.</p></td></tr>
<tr valign="top" align="left">
<td width="11%"></td>
<td width="1%">


<p>&bull;</p></td>
<td width="2%"></td>
<td width="86%">


<p>The new dlm cluster must be running on the host making
the change. The current dlm cluster name can be seen by:</p></td></tr>
</table>

<p style="margin-left:14%;">cat
/sys/kernel/config/dlm/cluster/cluster_name</p>

<table width="100%" border="0" rules="none" frame="void"
       cellspacing="0" cellpadding="0">
<tr valign="top" align="left">
<td width="11%"></td>
<td width="1%">


<p style="margin-top: 1em">&bull;</p></td>
<td width="2%"></td>
<td width="49%">


<p style="margin-top: 1em">Change the VG lock type to
none:</p> </td>
<td width="37%">
</td></tr>
</table>

<p style="margin-left:14%;">vgchange
&minus;&minus;lock&minus;type none
&minus;&minus;lock&minus;opt force &lt;vgname&gt;</p>

<table width="100%" border="0" rules="none" frame="void"
       cellspacing="0" cellpadding="0">
<tr valign="top" align="left">
<td width="11%"></td>
<td width="1%">


<p style="margin-top: 1em">&bull;</p></td>
<td width="2%"></td>
<td width="86%">


<p style="margin-top: 1em">Change the VG lock type back to
dlm which sets the new cluster name:</p></td></tr>
</table>

<p style="margin-left:14%;">vgchange
&minus;&minus;lock&minus;type dlm &lt;vgname&gt;</p>

<table width="100%" border="0" rules="none" frame="void"
       cellspacing="0" cellpadding="0">
<tr valign="top" align="left">
<td width="11%"></td>
<td width="1%">


<p style="margin-top: 1em">&bull;</p></td>
<td width="2%"></td>
<td width="49%">


<p style="margin-top: 1em">Start the VG on hosts to use
it:</p> </td>
<td width="37%">
</td></tr>
</table>

<p style="margin-left:14%;">vgchange
&minus;&minus;lock&minus;start &lt;vgname&gt;</p>

<p style="margin-left:11%; margin-top: 1em"><b>Changing a
local VG to a shared VG</b> <br>
All LVs must be inactive to change the lock type.</p>

<p style="margin-left:11%; margin-top: 1em">lvmlockd must
be configured and running as described in USAGE.</p>

<table width="100%" border="0" rules="none" frame="void"
       cellspacing="0" cellpadding="0">
<tr valign="top" align="left">
<td width="11%"></td>
<td width="1%">


<p style="margin-top: 1em">&bull;</p></td>
<td width="2%"></td>
<td width="77%">


<p style="margin-top: 1em">Change a local VG to a shared VG
with the command:</p></td>
<td width="9%">
</td></tr>
</table>

<p style="margin-left:14%;">vgchange
&minus;&minus;lock&minus;type sanlock|dlm &lt;vgname&gt;</p>

<table width="100%" border="0" rules="none" frame="void"
       cellspacing="0" cellpadding="0">
<tr valign="top" align="left">
<td width="11%"></td>
<td width="1%">


<p style="margin-top: 1em">&bull;</p></td>
<td width="2%"></td>
<td width="49%">


<p style="margin-top: 1em">Start the VG on hosts to use
it:</p> </td>
<td width="37%">
</td></tr>
</table>

<p style="margin-left:14%;">vgchange
&minus;&minus;lock&minus;start &lt;vgname&gt;</p>

<p style="margin-left:11%; margin-top: 1em"><b>Changing a
shared VG to a local VG</b> <br>
All LVs must be inactive to change the lock type.</p>

<table width="100%" border="0" rules="none" frame="void"
       cellspacing="0" cellpadding="0">
<tr valign="top" align="left">
<td width="11%"></td>
<td width="1%">


<p style="margin-top: 1em">&bull;</p></td>
<td width="2%"></td>
<td width="66%">


<p style="margin-top: 1em">Start the VG on the host making
the change:</p></td>
<td width="20%">
</td></tr>
</table>

<p style="margin-left:14%;">vgchange
&minus;&minus;lock&minus;start &lt;vgname&gt;</p>

<table width="100%" border="0" rules="none" frame="void"
       cellspacing="0" cellpadding="0">
<tr valign="top" align="left">
<td width="11%"></td>
<td width="1%">


<p style="margin-top: 1em">&bull;</p></td>
<td width="2%"></td>
<td width="48%">


<p style="margin-top: 1em">Stop the VG on all other
hosts:</p> </td>
<td width="38%">
</td></tr>
</table>

<p style="margin-left:14%;">vgchange
&minus;&minus;lock&minus;stop &lt;vgname&gt;</p>

<table width="100%" border="0" rules="none" frame="void"
       cellspacing="0" cellpadding="0">
<tr valign="top" align="left">
<td width="11%"></td>
<td width="1%">


<p style="margin-top: 1em">&bull;</p></td>
<td width="2%"></td>
<td width="86%">


<p style="margin-top: 1em">Change the VG lock type to none
on the host where the VG is started:</p></td></tr>
</table>

<p style="margin-left:14%;">vgchange
&minus;&minus;lock&minus;type none &lt;vgname&gt;</p>

<p style="margin-left:11%; margin-top: 1em">If the VG
cannot be started with the previous lock type, then the lock
type can be forcibly changed to none with: <br>
vgchange &minus;&minus;lock&minus;type none
&minus;&minus;lock&minus;opt force &lt;vgname&gt;</p>

<p style="margin-left:11%; margin-top: 1em">To change a VG
from one lock type to another (i.e. between sanlock and
dlm), first change it to a local VG, then to the new
type.</p>

<p style="margin-left:11%; margin-top: 1em"><b>Changing a
clvm/clustered VG to a shared VG</b> <br>
All LVs must be inactive to change the lock type.</p>

<p style="margin-left:11%; margin-top: 1em">First change
the clvm/clustered VG to a local VG. Within a running clvm
cluster, change a clustered VG to a local VG with the
command:</p>

<p style="margin-left:11%; margin-top: 1em">vgchange
&minus;cn &lt;vgname&gt;</p>

<p style="margin-left:11%; margin-top: 1em">If the clvm
cluster is no longer running on any nodes, then extra
options can be used to forcibly make the VG local. Caution:
this is only safe if all nodes have stopped using the
VG:</p>

<p style="margin-left:11%; margin-top: 1em">vgchange
&minus;&minus;lock&minus;type none
&minus;&minus;lock&minus;opt force &lt;vgname&gt;</p>

<p style="margin-left:11%; margin-top: 1em">After the VG is
local, follow the steps described in &quot;changing a local
VG to a shared VG&quot;.</p>

<p style="margin-left:11%; margin-top: 1em"><b>Extending an
LV active on multiple hosts</b> <br>
With lvmlockd and dlm, a special clustering procedure is
used to refresh a shared LV on remote cluster nodes after it
has been extended on one node.</p>

<p style="margin-left:11%; margin-top: 1em">When an LV
holding gfs2 or ocfs2 is active on multiple hosts with a
shared lock, lvextend is permitted to run with an existing
shared LV lock in place of the normal exclusive LV lock.</p>

<p style="margin-left:11%; margin-top: 1em">After lvextend
has finished extending the LV, it sends a remote request to
other nodes running the dlm to run &rsquo;lvchange
&minus;&minus;refresh&rsquo; on the LV. This uses
dlm_controld and corosync features.</p>

<p style="margin-left:11%; margin-top: 1em">Some special
&minus;&minus;lockopt values can be used to modify this
process. &quot;shupdate&quot; permits the lvextend update
with an existing shared lock if it isn&rsquo;t otherwise
permitted. &quot;norefresh&quot; prevents the remote refresh
operation.</p>

<p style="margin-left:11%; margin-top: 1em"><b>Limitations
of shared VGs</b> <br>
Things that do not yet work in shared VGs: <br>
&bull; using external origins for thin LVs <br>
&bull; splitting snapshots from LVs <br>
&bull; splitting mirrors in sanlock VGs <br>
&bull; pvmove of entire PVs, or under LVs activated with
shared locks <br>
&bull; vgsplit and vgmerge (convert to a local VG to do
this)</p>

<p style="margin-left:11%; margin-top: 1em"><b>lvmlockd
changes from clvmd</b> <br>
(See above for converting an existing clvm VG to a shared
VG.)</p>

<p style="margin-left:11%; margin-top: 1em">While lvmlockd
and clvmd are entirely different systems, LVM command usage
remains similar. Differences are more notable when using
lvmlockd&rsquo;s sanlock option.</p>

<p style="margin-left:11%; margin-top: 1em">Visible usage
differences between shared VGs (using lvmlockd) and
clvm/clustered VGs (using clvmd):</p>

<table width="100%" border="0" rules="none" frame="void"
       cellspacing="0" cellpadding="0">
<tr valign="top" align="left">
<td width="11%"></td>
<td width="1%">


<p style="margin-top: 1em">&bull;</p></td>
<td width="2%"></td>
<td width="86%">


<p style="margin-top: 1em">lvm.conf is configured to use
lvmlockd by setting use_lvmlockd=1. clvmd used
locking_type=3.</p> </td></tr>
<tr valign="top" align="left">
<td width="11%"></td>
<td width="1%">


<p>&bull;</p></td>
<td width="2%"></td>
<td width="86%">


<p>vgcreate &minus;&minus;shared creates a shared VG.
vgcreate &minus;&minus;clustered y created a clvm/clustered
VG.</p> </td></tr>
<tr valign="top" align="left">
<td width="11%"></td>
<td width="1%">


<p>&bull;</p></td>
<td width="2%"></td>
<td width="86%">


<p>lvmlockd adds the option of using sanlock for locking,
avoiding the need for network clustering.</p></td></tr>
<tr valign="top" align="left">
<td width="11%"></td>
<td width="1%">


<p>&bull;</p></td>
<td width="2%"></td>
<td width="86%">


<p>lvmlockd defaults to the exclusive activation mode
whenever the activation mode is unspecified, i.e. &minus;ay
means &minus;aey, not &minus;asy.</p></td></tr>
<tr valign="top" align="left">
<td width="11%"></td>
<td width="1%">


<p>&bull;</p></td>
<td width="2%"></td>
<td width="86%">


<p>lvmlockd commands always apply to the local host, and
never have an effect on a remote host. (The activation
option &rsquo;l&rsquo; is not used.)</p></td></tr>
<tr valign="top" align="left">
<td width="11%"></td>
<td width="1%">


<p>&bull;</p></td>
<td width="2%"></td>
<td width="86%">


<p>lvmlockd saves the cluster name for a shared VG using
dlm. Only hosts in the matching cluster can use the VG.</p></td></tr>
<tr valign="top" align="left">
<td width="11%"></td>
<td width="1%">


<p>&bull;</p></td>
<td width="2%"></td>
<td width="86%">


<p>lvmlockd requires starting/stopping shared VGs with
vgchange &minus;&minus;lock&minus;start and
&minus;&minus;lock&minus;stop.</p> </td></tr>
<tr valign="top" align="left">
<td width="11%"></td>
<td width="1%">


<p>&bull;</p></td>
<td width="2%"></td>
<td width="86%">


<p>vgremove of a sanlock VG may fail indicating that all
hosts have not stopped the VG lockspace. Stop the VG on all
hosts using vgchange &minus;&minus;lock&minus;stop.</p></td></tr>
<tr valign="top" align="left">
<td width="11%"></td>
<td width="1%">


<p>&bull;</p></td>
<td width="2%"></td>
<td width="86%">


<p>vgreduce or pvmove of a PV in a sanlock VG will fail if
it holds the internal &quot;lvmlock&quot; LV that holds the
sanlock locks.</p></td></tr>
<tr valign="top" align="left">
<td width="11%"></td>
<td width="1%">


<p>&bull;</p></td>
<td width="2%"></td>
<td width="86%">


<p>lvmlockd uses lock retries instead of lock queueing, so
high lock contention may require increasing
global/lvmlockd_lock_retries to avoid transient lock
failures.</p> </td></tr>
<tr valign="top" align="left">
<td width="11%"></td>
<td width="1%">


<p>&bull;</p></td>
<td width="2%"></td>
<td width="86%">


<p>lvmlockd includes VG reporting options lock_type and
lock_args, and LV reporting option lock_args to view the
corresponding metadata fields.</p></td></tr>
<tr valign="top" align="left">
<td width="11%"></td>
<td width="1%">


<p>&bull;</p></td>
<td width="2%"></td>
<td width="86%">


<p>In the &rsquo;vgs&rsquo; command&rsquo;s sixth VG attr
field, &quot;s&quot; for &quot;shared&quot; is displayed for
shared VGs.</p></td></tr>
<tr valign="top" align="left">
<td width="11%"></td>
<td width="1%">


<p>&bull;</p></td>
<td width="2%"></td>
<td width="86%">


<p>If lvmlockd fails or is killed while in use, locks it
held remain but are orphaned in the lock manager. lvmlockd
can be restarted with an option to adopt the orphan locks
from the previous instance of lvmlockd.</p></td></tr>
<tr valign="top" align="left">
<td width="11%"></td>
<td width="1%">


<p>&bull;</p></td>
<td width="2%"></td>
<td width="86%">


<p>The &rsquo;lvs&rsquo; command does not report any remote
state, because lvmlockd is unable to passively check the
remote active or lock state of an LV.</p></td></tr>
</table>

<h2>SEE ALSO
<a name="SEE ALSO"></a>
</h2>


<p style="margin-left:11%; margin-top: 1em"><b>lvm</b>(8),
<b>lvmlockctl</b>(8)</p>
<hr>
</body>
</html>
